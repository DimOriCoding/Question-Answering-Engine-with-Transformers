{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**I import some useful libraries in python.**"
      ],
      "metadata": {
        "id": "ktn6WZIbBp5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqFOGp2zILSz",
        "outputId": "2561d1bf-55e5-4b90-9a40-3faa07a58014"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsXzP8XpIsQ"
      },
      "source": [
        "**I mount google drive because all the files (for example .csv) that i used for the implementation of my thesis are saved on my google drive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qShrK9odXOrW",
        "outputId": "a93d048a-0f62-42b5-a6c6-1fd2e71ca8b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61VZ1wZC6qfa"
      },
      "source": [
        "**Reading the training and validation datasets that are in .txt file format and droping of the column of their answers ids.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNFK45c66TcK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_dataset = pd.read_csv('annotated_wd_data_train_answerable.txt', sep=\"\\t\", names=['entity_id','relation_id', 'answer', 'Question']) #read train dataset\n",
        "validation_dataset = pd.read_csv('annotated_wd_data_valid_answerable.txt', sep=\"\\t\", names=['entity_id','relation_id', 'answer', 'Question'])#read validation dataset\n",
        "train_dataset = train_dataset.drop(labels='answer', axis=1) #drop answer\n",
        "validation_dataset = validation_dataset.drop(labels='answer', axis=1) #drop answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I made some preprocessing for each question by creating the function preprocessing, which takes as argument the text (in this case the question) and performs accent removal, as well as punctuation mark removal (period, question mark, exclamation mark), s removal with apostrophe('s) and comma(,) removal.Finally this function, which is shown below, returns the original text with the above mentioned processing.**"
      ],
      "metadata": {
        "id": "BCFzPH1ltm3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yGYRFAmXUyg"
      },
      "outputs": [],
      "source": [
        "def Preprocessing(text):\n",
        "    text = unidecode(text.replace(\"?\", \"\").replace(\"'s\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\",\", \"\"))\n",
        "    return text\n",
        "\n",
        "train_dataset['Question'] = train_dataset['Question'].apply(lambda x:Preprocessing(x))\n",
        "validation_dataset['Question'] = validation_dataset['Question'].apply(lambda x:Preprocessing(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The SPARQLWrapper library is downloaded, which contributes to the generation of SPARQL queries, in order to find the entity labels.**"
      ],
      "metadata": {
        "id": "KXmlWHObtyuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WQm8KmD7hhY",
        "outputId": "2b070690-9dea-428a-9a80-1ec69829f4d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sparqlwrapper in /usr/local/lib/python3.10/dist-packages (2.0.0)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.10/dist-packages (from sparqlwrapper) (7.0.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->sparqlwrapper) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->sparqlwrapper) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->sparqlwrapper) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#agent='WDQS-example Python'\n",
        "#Chicobot-Agent\n",
        "!pip install sparqlwrapper\n",
        "import sys\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent= \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The function find\\_divisors which receives as an argument the length of either the SimpleQuestions training or the SimpleQuestions validation dataset and returns the divisors of this number, has as aim to find the divisors of the lengths of of the full SimpleQuestions training and validation dataset, so that it is then possible to get the entity labels from SPARQL as quickly as possible.**"
      ],
      "metadata": {
        "id": "5qt4QghWVy6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_divisors(a):\n",
        "    divisors = []\n",
        "    for i in range(1, a + 1):\n",
        "        if a % i == 0:\n",
        "            divisors.append(i)\n",
        "    return divisors\n",
        "\n",
        "# Example usage\n",
        "number_to_check_1 = len(train_dataset)\n",
        "number_to_check_2 = len(validation_dataset)\n",
        "result_1 = find_divisors(number_to_check_1)\n",
        "result_2 = find_divisors(number_to_check_2)\n",
        "print(f\"The divisors of {number_to_check_1} are: {result_1}\")\n",
        "print(f\"The divisors of {number_to_check_2} are: {result_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N8VQP47g5uq",
        "outputId": "116135d1-2487-40c8-f896-e3e690c7a4e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The divisors of 19481 are: [1, 7, 11, 23, 77, 121, 161, 253, 847, 1771, 2783, 19481]\n",
            "The divisors of 2821 are: [1, 7, 13, 31, 91, 217, 403, 2821]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXobjE7a7K3m"
      },
      "source": [
        "**The SPARQL Queries below found from wikidata the entity labels and the entity ids for each entity id (of course for each question) of the SimpleQuestions training και validation dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5yt-fziZO5j"
      },
      "outputs": [],
      "source": [
        "train_entity_names = []\n",
        "train_entity_ids = []\n",
        "for i in range(0,len(train_dataset),102):  #retrieve training set entities' corresponding ids\n",
        "  str = \"\"\n",
        "  for entity_id in train_dataset['entity_id'][i:i+161]:\n",
        "   str = str + \"wd:\" + entity_id + \" \"\n",
        "  sparql.setQuery(\"\"\"\n",
        "  SELECT ?item ?itemLabel\n",
        "  WHERE\n",
        "  {\n",
        "    VALUES ?item {\"\"\" + str + \"\"\"}\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "\"\"\")\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "  train_results_df = pd.json_normalize(results['results']['bindings'])\n",
        "  for j in train_results_df['item.value']:\n",
        "      train_entity_ids.append(j)\n",
        "  for j in train_results_df['itemLabel.value']:\n",
        "      train_entity_names.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ru4Qc9h1FBX"
      },
      "outputs": [],
      "source": [
        "validation_entity_names = []\n",
        "validation_entity_ids = []\n",
        "for i in range(0,len(validation_dataset),157):  #retrieve validation set entity's corresponding ids\n",
        "  str = \"\"\n",
        "  for entity_id in validation_dataset['entity_id'][i:i+217]:\n",
        "   str = str + \"wd:\" + entity_id + \" \"\n",
        "  sparql.setQuery(\"\"\"\n",
        "  SELECT ?item ?itemLabel\n",
        "  WHERE\n",
        "  {\n",
        "    VALUES ?item {\"\"\" + str + \"\"\"}\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "\"\"\")\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "  validation_results_df = pd.json_normalize(results['results']['bindings'])\n",
        "  for j in validation_results_df['item.value']:\n",
        "      validation_entity_ids.append(j)\n",
        "  for j in validation_results_df['itemLabel.value']:\n",
        "      validation_entity_names.append(j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P743BCFh69j-"
      },
      "source": [
        "**Creation of the corresponding lists containing the questions, the entity labels, the relation ids and the entity ids of the initial SimpleQuestions training and validation dataset, either this dataset includes answerable and answerable questions over wikidata questions or only answerable questions over wikidata.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_questions = train_dataset['Question'].to_list()\n",
        "validation_dataset_questions = validation_dataset['Question'].to_list()\n",
        "train_dataset_entity_ids = train_dataset['entity_id'].to_list()\n",
        "validation_dataset_entity_ids = validation_dataset['entity_id'].to_list()"
      ],
      "metadata": {
        "id": "ga5QHwHi8eGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jrj4q-di74iH"
      },
      "source": [
        "**The entity ids that are occurred from SPARQL are expressions that contain an encoded name, for example for the question of the SimpleQuestions (full) training dataset \"who is a musician born in detroit\", the entity id that occurred is https://www.wikidata.org/wiki/Q12439, which is related with Detroit (Q12439, if i search in wikidata).**\n",
        "\n",
        "**So the python function entity\\_ids is created with the aim to split the full expression with the entity id, such as the example above and to keep the encoded name, i.e named start from \"Q\" (\"Q12439\" in the above example).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WBxpijLGuTA",
        "outputId": "b6941830-8f00-4a20-93d4-8a8140787780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19481\n",
            "31315\n",
            "2821\n",
            "3993\n"]
        }
      ],
      "source": [
        "#Split Spaqrl URI's in order to get the entity ids\n",
        "import os\n",
        "def enitity_ids(ids): #get ids from sparql query ids\n",
        " entity_ids = []\n",
        " for i in range(len(ids)):\n",
        "   entity_ids.append(ids[i].split(os.path.sep)[-1])\n",
        " return entity_ids\n",
        "train_ids = enitity_ids(train_entity_ids)\n",
        "print(len(train_dataset['entity_id']))\n",
        "print(len(train_ids))\n",
        "validation_ids = enitity_ids(validation_entity_ids)\n",
        "print(len(validation_dataset['entity_id']))\n",
        "print(len(validation_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xum0V4uGCmpQ"
      },
      "source": [
        "**The function labels from which i confirm the entity id of each question, which occurred from SPARQL, through the entity id obtained for each question from the transfer of SimpleQuestions from Freebase to Wikidata. This function takes as arguments the list of questions, the list of the entity ids both from the corresponding SimpleQuestions dataset column and from the entity\\_ids function and the list of entity labels occured from SPARQL.**\n",
        "\n",
        "**Essentially i assigned to each question and entity id its corresponding entity label by searching in the list of training and validation entity ids obtained from SPARQL. If any of these entity ids exist in the SimpleQuestions training and validation entity labels lists, that are created before and matching the entity label and thus a new column with entity labels for the training and the validation dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import operator as op\n",
        "def labels(questions, entity_id_dataset, entity_ids, entity_names):# entity_ids and dataset must have the same length\n",
        "#In this step we find the entity ids from the initial dataset to the list occured from Sparql Query\n",
        "#for each of the train and validation dataset.\n",
        "#This will become the train or validation data new column, containing entities' labels\n",
        "\n",
        "  label_df_column = []\n",
        "  for entity_id in entity_id_dataset:\n",
        "   j=0\n",
        "   for ent in entity_ids: #find its id on list\n",
        "    if entity_id == ent:\n",
        "      label_df_column.append(entity_names[j]) #and find corresponding label\n",
        "      break\n",
        "    j += 1\n",
        "  return label_df_column\n",
        "\n",
        "\n",
        "train_dataset['entity_label'] = labels(train_dataset_questions, train_dataset_entity_ids, train_ids, train_entity_names)\n",
        "validation_dataset['entity_label'] = labels(validation_dataset_questions, validation_dataset_entity_ids ,validation_ids, validation_entity_names)\n",
        "print(train_dataset['entity_label'], len(train_dataset['entity_label']))\n",
        "print(validation_dataset['entity_label'], len(validation_dataset['entity_label']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuuV-rzY03gK",
        "outputId": "a5a0d04a-3bda-4db0-e020-4be5731c8331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0                 Detroit\n",
            "1             Mera Shikar\n",
            "2                 Chicago\n",
            "3              midfielder\n",
            "4           Mike Twellman\n",
            "               ...       \n",
            "19476      mountain tapir\n",
            "19477      superhero film\n",
            "19478         comedy film\n",
            "19479      Anthony Bailey\n",
            "19480    Gastón Filgueira\n",
            "Name: entity_label, Length: 19481, dtype: object 19481\n",
            "0                Sasha Vujačić\n",
            "1            Wiebke Carolsfeld\n",
            "2       Seymour Parker Gilbert\n",
            "3             Antoine de Févin\n",
            "4                Jamie Hewlett\n",
            "                 ...          \n",
            "2816              OutNumbered!\n",
            "2817            Seumas O'Kelly\n",
            "2818            Herby Fortunat\n",
            "2819         Nikolaj Frobenius\n",
            "2820                 São Paulo\n",
            "Name: entity_label, Length: 2821, dtype: object 2821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ATPSMnMgzwX"
      },
      "source": [
        "**The preprocessing of the entity labels column occurred from labels function is made through function Preprocessing that is created previously. The reindexing of the columns (entity id, relation id, questions, entity label) of the initial SimpleQuestions dataset (either full or the dataset containing only questions that are answerable over wikidata),in order to include the entity labels as a new column in the initial SimpleQuestions dataset, is achieved by using the reindex() method from pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1lTZMA-F_5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cfb3f1-0a8a-4cb7-edf1-4e7eaf745321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       entity_id      entity_label relation_id  \\\n",
            "0         Q12439           Detroit         R19   \n",
            "1       Q6817891       Mera Shikar        P364   \n",
            "2          Q1297           Chicago        R276   \n",
            "3        Q193592        midfielder        R413   \n",
            "4       Q6849115     Mike Twellman        P413   \n",
            "...          ...               ...         ...   \n",
            "19476    Q223960    mountain tapir        P171   \n",
            "19477   Q1535153    superhero film        R136   \n",
            "19478    Q157443       comedy film        R136   \n",
            "19479  Q16093542    Anthony Bailey         P27   \n",
            "19480    Q926822  Gaston Filgueira         P21   \n",
            "\n",
            "                                                Question  \n",
            "0                      who is a musician born in detroit  \n",
            "1      what is the language in which mera shikar was ...  \n",
            "2      Whats the name of a battle that happened in ch...  \n",
            "3              what player plays the position midfielder  \n",
            "4         what is the position that  mike twellman plays  \n",
            "...                                                  ...  \n",
            "19476  what classification does  mountain tapir come ...  \n",
            "19477   What a superhero movie that premiered on toonami  \n",
            "19478  What is the name of a comedy film that is also...  \n",
            "19479          What is the nationality of anthony bailey  \n",
            "19480                    What gender is gaston filgueira  \n",
            "\n",
            "[19481 rows x 4 columns]\n",
            "     entity_id            entity_label relation_id  \\\n",
            "0      Q318926           Sasha Vujacic         P19   \n",
            "1     Q2568216       Wiebke Carolsfeld         R57   \n",
            "2     Q2275923  Seymour Parker Gilbert        P106   \n",
            "3     Q2856873        Antoine de Fevin         P20   \n",
            "4      Q522966           Jamie Hewlett        P106   \n",
            "...        ...                     ...         ...   \n",
            "2816  Q7111404             OutNumbered        P178   \n",
            "2817  Q3480757          Seumas O'Kelly         P21   \n",
            "2818  Q3709870          Herby Fortunat        P413   \n",
            "2819  Q2981169       Nikolaj Frobenius        P106   \n",
            "2820      Q174               Sao Paulo         R19   \n",
            "\n",
            "                                               Question  \n",
            "0                          where was sasha vujacic born  \n",
            "1          What is a film directed by wiebke carolsfeld  \n",
            "2            What was Seymour Parker Gilbert profession  \n",
            "3         in what french city did antoine de fevin die   \n",
            "4                      What job does jamie hewlett have  \n",
            "...                                                 ...  \n",
            "2816                          who developed outnumbered  \n",
            "2817                  what is the sex of seumas o'kelly  \n",
            "2818  which position did herby fortunat play in foot...  \n",
            "2819               What is nikolaj frobenius profession  \n",
            "2820         who is a person that was born in sao paulo  \n",
            "\n",
            "[2821 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "train_dataset['entity_label'] = train_dataset['entity_label'].apply(lambda x:Preprocessing(x))\n",
        "validation_dataset['entity_label'] = validation_dataset['entity_label'].apply(lambda x:Preprocessing(x))\n",
        "new_columns = [\"entity_id\",\"entity_label\",\"relation_id\",\"Question\"]\n",
        "train_dataset = train_dataset.reindex(columns=new_columns)\n",
        "validation_dataset = validation_dataset.reindex(columns=new_columns)\n",
        "print(train_dataset)\n",
        "print(validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creation of the corresponding .csv files for the SimpleQuestions training and validation dataset respectively through pandas method to_csv().**"
      ],
      "metadata": {
        "id": "PyYGlHih8rcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Specify the file name\n",
        "train_csv_name = \"drive/MyDrive/train_answerable_questions_dataset.csv\"\n",
        "# Writing to CSV file\n",
        "train_dataset.to_csv(train_csv_name, index=False)\n",
        "\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_gPpwIlaTcb",
        "outputId": "c16a218b-11da-4472-e19c-da1c0569ea28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       entity_id      entity_label relation_id  \\\n",
            "0         Q12439           Detroit         R19   \n",
            "1       Q6817891       Mera Shikar        P364   \n",
            "2          Q1297           Chicago        R276   \n",
            "3        Q193592        midfielder        R413   \n",
            "4       Q6849115     Mike Twellman        P413   \n",
            "...          ...               ...         ...   \n",
            "19476    Q223960    mountain tapir        P171   \n",
            "19477   Q1535153    superhero film        R136   \n",
            "19478    Q157443       comedy film        R136   \n",
            "19479  Q16093542    Anthony Bailey         P27   \n",
            "19480    Q926822  Gaston Filgueira         P21   \n",
            "\n",
            "                                                Question  \n",
            "0                      who is a musician born in detroit  \n",
            "1      what is the language in which mera shikar was ...  \n",
            "2      Whats the name of a battle that happened in ch...  \n",
            "3              what player plays the position midfielder  \n",
            "4         what is the position that  mike twellman plays  \n",
            "...                                                  ...  \n",
            "19476  what classification does  mountain tapir come ...  \n",
            "19477   What a superhero movie that premiered on toonami  \n",
            "19478  What is the name of a comedy film that is also...  \n",
            "19479          What is the nationality of anthony bailey  \n",
            "19480                    What gender is gaston filgueira  \n",
            "\n",
            "[19481 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_csv_name = \"drive/MyDrive/valid_answerable_questions_dataset.csv\"\n",
        "validation_dataset.to_csv(valid_csv_name, index=False)\n",
        "\n",
        "\n",
        "print(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hPbMfUNa2VB",
        "outputId": "4290528b-505b-40d0-a9b2-a99973ba68e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     entity_id            entity_label relation_id  \\\n",
            "0      Q318926           Sasha Vujacic         P19   \n",
            "1     Q2568216       Wiebke Carolsfeld         R57   \n",
            "2     Q2275923  Seymour Parker Gilbert        P106   \n",
            "3     Q2856873        Antoine de Fevin         P20   \n",
            "4      Q522966           Jamie Hewlett        P106   \n",
            "...        ...                     ...         ...   \n",
            "2816  Q7111404             OutNumbered        P178   \n",
            "2817  Q3480757          Seumas O'Kelly         P21   \n",
            "2818  Q3709870          Herby Fortunat        P413   \n",
            "2819  Q2981169       Nikolaj Frobenius        P106   \n",
            "2820      Q174               Sao Paulo         R19   \n",
            "\n",
            "                                               Question  \n",
            "0                          where was sasha vujacic born  \n",
            "1          What is a film directed by wiebke carolsfeld  \n",
            "2            What was Seymour Parker Gilbert profession  \n",
            "3         in what french city did antoine de fevin die   \n",
            "4                      What job does jamie hewlett have  \n",
            "...                                                 ...  \n",
            "2816                          who developed outnumbered  \n",
            "2817                  what is the sex of seumas o'kelly  \n",
            "2818  which position did herby fortunat play in foot...  \n",
            "2819               What is nikolaj frobenius profession  \n",
            "2820         who is a person that was born in sao paulo  \n",
            "\n",
            "[2821 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}
