{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**I import some useful libraries in python.**"
      ],
      "metadata": {
        "id": "ktn6WZIbBp5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqFOGp2zILSz",
        "outputId": "87b2da20-9582-4ac1-9291-91fb4c4052f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/235.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/235.5 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "!pip install unidecode\n",
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTsXzP8XpIsQ"
      },
      "source": [
        "**I mount google drive because all the files (for example .csv) that i used for the implementation of my thesis are saved on my google drive.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qShrK9odXOrW",
        "outputId": "7da925b8-0777-4a2d-c577-5da06b2d0407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCO0xDqXfX6M"
      },
      "source": [
        "**Reading the training and validation datasets that are in .txt file format and droping of the column of their answers ids.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNFK45c66TcK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_dataset = pd.read_csv('annotated_wd_data_train.txt', sep=\"\\t\", names=['entity_id','relation_id', 'answer', 'Question']) #read train dataset\n",
        "validation_dataset = pd.read_csv('annotated_wd_data_valid.txt', sep=\"\\t\", names=['entity_id','relation_id', 'answer', 'Question'])#read validation dataset\n",
        "train_dataset = train_dataset.drop(labels='answer', axis=1) #drop answer\n",
        "validation_dataset = validation_dataset.drop(labels='answer', axis=1) #drop answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**I made some preprocessing for each question by creating the function preprocessing, which takes as argument the text (in this case the question) and performs accent removal, as well as punctuation mark removal (period, question mark, exclamation mark), s removal with apostrophe('s) and comma(,) removal.Finally this function, which is shown below, returns the original text with the above mentioned processing.**"
      ],
      "metadata": {
        "id": "BCFzPH1ltm3u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yGYRFAmXUyg"
      },
      "outputs": [],
      "source": [
        "def Preprocessing(text):\n",
        "    text = unidecode(text.replace(\"?\", \"\").replace(\"'s\", \"\").replace(\".\", \"\").replace(\"!\", \"\").replace(\",\", \"\"))\n",
        "    return text\n",
        "\n",
        "train_dataset['Question'] = train_dataset['Question'].apply(lambda x:Preprocessing(x))\n",
        "validation_dataset['Question'] = validation_dataset['Question'].apply(lambda x:Preprocessing(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The SPARQLWrapper library is downloaded, which contributes to the generation of SPARQL queries, in order to find the entity labels.**"
      ],
      "metadata": {
        "id": "KXmlWHObtyuO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WQm8KmD7hhY",
        "outputId": "851113b2-e3a8-4217-dd0a-227870392e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sparqlwrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Collecting rdflib>=6.1.1 (from sparqlwrapper)\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib>=6.1.1->sparqlwrapper)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib>=6.1.1->sparqlwrapper) (3.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib>=6.1.1->sparqlwrapper) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib, sparqlwrapper\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0 sparqlwrapper-2.0.0\n"
          ]
        }
      ],
      "source": [
        "#agent='WDQS-example Python'\n",
        "#Chicobot-Agent\n",
        "!pip install sparqlwrapper\n",
        "import sys\n",
        "from SPARQLWrapper import SPARQLWrapper, JSON, CSV\n",
        "sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\", agent= \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The function find\\_divisors which receives as an argument the length of either the SimpleQuestions training or the SimpleQuestions validation dataset and returns the divisors of this number, has as aim to find the divisors of the lengths of of the full SimpleQuestions training and validation dataset, so that it is then possible to get the entity labels from SPARQL as quickly as possible.**"
      ],
      "metadata": {
        "id": "5qt4QghWVy6o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_divisors(a):\n",
        "    divisors = []\n",
        "    for i in range(1, a + 1):\n",
        "        if a % i == 0:\n",
        "            divisors.append(i)\n",
        "    return divisors\n",
        "\n",
        "# Example usage\n",
        "number_to_check_1 = 34374\n",
        "number_to_check_2 = 4867\n",
        "result_1 = find_divisors(number_to_check_1)\n",
        "result_2 = find_divisors(number_to_check_2)\n",
        "print(f\"The divisors of {number_to_check_1} are: {result_1}\")\n",
        "print(f\"The divisors of {number_to_check_2} are: {result_2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N8VQP47g5uq",
        "outputId": "136dbf18-8c6e-4413-d6f2-81dbd99b9211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The divisors of 34374 are: [1, 2, 3, 6, 17, 34, 51, 102, 337, 674, 1011, 2022, 5729, 11458, 17187, 34374]\n",
            "The divisors of 4867 are: [1, 31, 157, 4867]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok_9ruOxgGk-"
      },
      "source": [
        "**The SPARQL Queries below found from wikidata the entity labels and the entity ids for each entity id (of course for each question) of the SimpleQuestions training και validation dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5yt-fziZO5j"
      },
      "outputs": [],
      "source": [
        "train_entity_names = []\n",
        "train_entity_ids = []\n",
        "for i in range(0,len(train_dataset),102):  #retrieve training set entities' corresponding ids\n",
        "  str = \"\"\n",
        "  for entity_id in train_dataset['entity_id'][i:i+102]:\n",
        "   str = str + \"wd:\" + entity_id + \" \"\n",
        "  sparql.setQuery(\"\"\"\n",
        "  SELECT ?item ?itemLabel\n",
        "  WHERE\n",
        "  {\n",
        "    VALUES ?item {\"\"\" + str + \"\"\"}\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "\"\"\")\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "  train_results_df = pd.json_normalize(results['results']['bindings'])\n",
        "  for j in train_results_df['item.value']:\n",
        "      train_entity_ids.append(j)\n",
        "  for j in train_results_df['itemLabel.value']:\n",
        "      train_entity_names.append(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ru4Qc9h1FBX"
      },
      "outputs": [],
      "source": [
        "validation_entity_names = []\n",
        "validation_entity_ids = []\n",
        "for i in range(0,len(validation_dataset),157):  #retrieve validation set entity's corresponding ids\n",
        "  str = \"\"\n",
        "  for entity_id in validation_dataset['entity_id'][i:i+157]:\n",
        "   str = str + \"wd:\" + entity_id + \" \"\n",
        "  sparql.setQuery(\"\"\"\n",
        "  SELECT ?item ?itemLabel\n",
        "  WHERE\n",
        "  {\n",
        "    VALUES ?item {\"\"\" + str + \"\"\"}\n",
        "    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n",
        "  }\n",
        "\"\"\")\n",
        "  sparql.setReturnFormat(JSON)\n",
        "  results = sparql.query().convert()\n",
        "  validation_results_df = pd.json_normalize(results['results']['bindings'])\n",
        "  for j in validation_results_df['item.value']:\n",
        "      validation_entity_ids.append(j)\n",
        "  for j in validation_results_df['itemLabel.value']:\n",
        "      validation_entity_names.append(j)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P743BCFh69j-"
      },
      "source": [
        "**Creation of the corresponding lists containing the questions, the entity labels, the relation ids and the entity ids of the corresponding SimpleQuestions training and validation dataset, either this dataset includes answerable and answerable questions over wikidata questions or only answerable questions over wikidata.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_questions = train_dataset['Question'].to_list()\n",
        "validation_dataset_questions = validation_dataset['Question'].to_list()\n",
        "train_dataset_entity_ids = train_dataset['entity_id'].to_list()\n",
        "validation_dataset_entity_ids = validation_dataset['entity_id'].to_list()"
      ],
      "metadata": {
        "id": "ga5QHwHi8eGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSfdP3qUgmvP"
      },
      "source": [
        "**The entity ids that are occurred from SPARQL are expressions that contain an encoded name, for example for the question of the SimpleQuestions (full) training dataset \"who is a musician born in detroit\", the entity id that occurred is https://www.wikidata.org/wiki/Q12439, which is related with Detroit (Q12439, if i search in wikidata).**\n",
        "\n",
        "**So the python function entity\\_ids is created with the aim to split the full expression with the entity id, such as the example above and to keep the encoded name, i.e named start from \"Q\" (\"Q12439\" in the above example).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WBxpijLGuTA",
        "outputId": "747eaa53-5da4-4112-bbaf-bc411a511e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34852\n",
            "34374\n",
            "4867\n",
            "4983\n"
          ]
        }
      ],
      "source": [
        "#Split Spaqrl URI's in order to get the entity ids\n",
        "import os\n",
        "def enitity_ids(ids): #get ids from sparql query ids\n",
        " entity_ids = []\n",
        " for i in range(len(ids)):\n",
        "   entity_ids.append(ids[i].split(os.path.sep)[-1])\n",
        " return entity_ids\n",
        "train_ids = enitity_ids(train_entity_ids)\n",
        "validation_ids = enitity_ids(validation_entity_ids)\n",
        "print(len(train_ids))\n",
        "print(len(train_dataset['entity_id']))\n",
        "print(len(validation_dataset['entity_id']))\n",
        "print(len(validation_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xum0V4uGCmpQ"
      },
      "source": [
        "**The function labels from which i confirm the entity id of each question, which occurred from SPARQL, through the entity id obtained for each question from the transfer of SimpleQuestions from Freebase to Wikidata. This function takes as arguments the list of questions, the list of the entity ids both from the corresponding SimpleQuestions dataset column and from the entity\\_ids function and the list of entity labels occured from SPARQL.**\n",
        "\n",
        "**Essentially i assigned to each question and entity id its corresponding entity label by searching in the list of training and validation entity ids obtained from SPARQL. If any of these entity ids exist in the SimpleQuestions training and validation entity labels lists, that are created before and matching the entity label and thus a new column with entity labels for the training and the validation dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import operator as op\n",
        "def labels(questions, entity_id_dataset, entity_ids, entity_names):# entity_ids and dataset must have the same length\n",
        "#In this step we find the entity ids from the initial dataset to the list occured from Sparql Query\n",
        "#for each of the train and validation dataset.\n",
        "#This will become the train or validation data new column, containing entities' labels\n",
        "\n",
        "  label_df_column = []\n",
        "  for entity_id in entity_id_dataset:\n",
        "   j=0\n",
        "   for ent in entity_ids: #find its id on list\n",
        "    if entity_id == ent:\n",
        "      label_df_column.append(entity_names[j]) #and find corresponding label\n",
        "      break\n",
        "    j += 1\n",
        "  return label_df_column\n",
        "\n",
        "\n",
        "train_dataset['entity_label'] = labels(train_dataset_questions, train_dataset_entity_ids, train_ids, train_entity_names)\n",
        "validation_dataset['entity_label'] = labels(validation_dataset_questions, validation_dataset_entity_ids ,validation_ids, validation_entity_names)\n",
        "print(train_dataset['entity_label'], len(train_dataset['entity_label']))\n",
        "print(validation_dataset['entity_label'], len(validation_dataset['entity_label']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuuV-rzY03gK",
        "outputId": "6752432b-aa7f-43d3-ff80-75435c686a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0            Warner Bros.\n",
            "1                 Detroit\n",
            "2                Q7370831\n",
            "3             Mera Shikar\n",
            "4                 Chicago\n",
            "               ...       \n",
            "34369      Anthony Bailey\n",
            "34370      Homi K. Bhabha\n",
            "34371    video game music\n",
            "34372    Gastón Filgueira\n",
            "34373            defender\n",
            "Name: entity_label, Length: 34374, dtype: object 34374\n",
            "0           JW Marriott Panama\n",
            "1                Sasha Vujačić\n",
            "2            Wiebke Carolsfeld\n",
            "3       Seymour Parker Gilbert\n",
            "4             Antoine de Févin\n",
            "                 ...          \n",
            "4862            Herby Fortunat\n",
            "4863               The Pianist\n",
            "4864                  Jon Seda\n",
            "4865         Nikolaj Frobenius\n",
            "4866                 São Paulo\n",
            "Name: entity_label, Length: 4867, dtype: object 4867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ATPSMnMgzwX"
      },
      "source": [
        "**The preprocessing of the entity labels column occurred from labels function is made through function Preprocessing that is created previously. The reindexing of the columns (entity id, relation id, questions, entity label) of the initial SimpleQuestions dataset (either full or the dataset containing only questions that are answerable over wikidata),in order to include the entity labels as a new column in the initial SimpleQuestions dataset, is achieved by using the reindex() method from pandas.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1lTZMA-F_5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea2e37b-40b3-40d8-baa7-7431db61fdd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       entity_id      entity_label relation_id  \\\n",
            "0        Q126399       Warner Bros        R272   \n",
            "1         Q12439           Detroit         R19   \n",
            "2       Q7370831          Q7370831        P162   \n",
            "3       Q6817891       Mera Shikar        P364   \n",
            "4          Q1297           Chicago        R276   \n",
            "...          ...               ...         ...   \n",
            "34369  Q16093542    Anthony Bailey         P27   \n",
            "34370    Q325741     Homi K Bhabha        P737   \n",
            "34371   Q1062702  video game music        R136   \n",
            "34372    Q926822  Gaston Filgueira         P21   \n",
            "34373    Q336286          defender        R413   \n",
            "\n",
            "                                                Question  \n",
            "0                  what movie is produced by warner bros  \n",
            "1                      who is a musician born in detroit  \n",
            "2                who produced the film rough house rosie  \n",
            "3      what is the language in which mera shikar was ...  \n",
            "4      Whats the name of a battle that happened in ch...  \n",
            "...                                                  ...  \n",
            "34369          What is the nationality of anthony bailey  \n",
            "34370    who was homi k bhabha especially influenced by   \n",
            "34371             which artist composes video game music  \n",
            "34372                    What gender is gaston filgueira  \n",
            "34373                      what player played a defender  \n",
            "\n",
            "[34374 rows x 4 columns]\n",
            "     entity_id            entity_label relation_id  \\\n",
            "0     Q3541144      JW Marriott Panama        P138   \n",
            "1      Q318926           Sasha Vujacic         P19   \n",
            "2     Q2568216       Wiebke Carolsfeld         R57   \n",
            "3     Q2275923  Seymour Parker Gilbert        P106   \n",
            "4     Q2856873        Antoine de Fevin         P20   \n",
            "...        ...                     ...         ...   \n",
            "4862  Q3709870          Herby Fortunat        P413   \n",
            "4863   Q150804             The Pianist        P136   \n",
            "4864  Q1343857                Jon Seda         P19   \n",
            "4865  Q2981169       Nikolaj Frobenius        P106   \n",
            "4866      Q174               Sao Paulo         R19   \n",
            "\n",
            "                                               Question  \n",
            "0     Who was the trump ocean club international hot...  \n",
            "1                          where was sasha vujacic born  \n",
            "2          What is a film directed by wiebke carolsfeld  \n",
            "3            What was Seymour Parker Gilbert profession  \n",
            "4         in what french city did antoine de fevin die   \n",
            "...                                                 ...  \n",
            "4862  which position did herby fortunat play in foot...  \n",
            "4863       what kind of film is the pianist (2002 film)  \n",
            "4864                     where was jon seda given birth  \n",
            "4865               What is nikolaj frobenius profession  \n",
            "4866         who is a person that was born in sao paulo  \n",
            "\n",
            "[4867 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "train_dataset['entity_label'] = train_dataset['entity_label'].apply(lambda x:Preprocessing(x))\n",
        "validation_dataset['entity_label'] = validation_dataset['entity_label'].apply(lambda x:Preprocessing(x))\n",
        "new_columns = [\"entity_id\",\"entity_label\",\"relation_id\",\"Question\"]\n",
        "train_dataset = train_dataset.reindex(columns=new_columns)\n",
        "validation_dataset = validation_dataset.reindex(columns=new_columns)\n",
        "print(train_dataset)\n",
        "print(validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creation of the corresponding .csv files for the SimpleQuestions training and validation dataset respectively through pandas method to_csv().**"
      ],
      "metadata": {
        "id": "z1uoyqIIaHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "# Specify the file name\n",
        "train_csv_name = \"drive/MyDrive/train_dataset.csv\"\n",
        "# Writing to CSV file\n",
        "train_dataset.to_csv(train_csv_name, index=False)\n",
        "\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_gPpwIlaTcb",
        "outputId": "35de0b06-c0ff-460a-88df-ee72d5b79d40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       entity_id      entity_label relation_id  \\\n",
            "0        Q126399       Warner Bros        R272   \n",
            "1         Q12439           Detroit         R19   \n",
            "2       Q7370831          Q7370831        P162   \n",
            "3       Q6817891       Mera Shikar        P364   \n",
            "4          Q1297           Chicago        R276   \n",
            "...          ...               ...         ...   \n",
            "34369  Q16093542    Anthony Bailey         P27   \n",
            "34370    Q325741     Homi K Bhabha        P737   \n",
            "34371   Q1062702  video game music        R136   \n",
            "34372    Q926822  Gaston Filgueira         P21   \n",
            "34373    Q336286          defender        R413   \n",
            "\n",
            "                                                Question  \n",
            "0                  what movie is produced by warner bros  \n",
            "1                      who is a musician born in detroit  \n",
            "2                who produced the film rough house rosie  \n",
            "3      what is the language in which mera shikar was ...  \n",
            "4      Whats the name of a battle that happened in ch...  \n",
            "...                                                  ...  \n",
            "34369          What is the nationality of anthony bailey  \n",
            "34370    who was homi k bhabha especially influenced by   \n",
            "34371             which artist composes video game music  \n",
            "34372                    What gender is gaston filgueira  \n",
            "34373                      what player played a defender  \n",
            "\n",
            "[34374 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_csv_name = \"drive/MyDrive/valid_dataset.csv\"\n",
        "validation_dataset.to_csv(valid_csv_name, index=False)\n",
        "\n",
        "\n",
        "print(validation_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hPbMfUNa2VB",
        "outputId": "2577eb06-1c54-42c8-e3a4-9a1fcbf7014b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     entity_id            entity_label relation_id  \\\n",
            "0     Q3541144      JW Marriott Panama        P138   \n",
            "1      Q318926           Sasha Vujacic         P19   \n",
            "2     Q2568216       Wiebke Carolsfeld         R57   \n",
            "3     Q2275923  Seymour Parker Gilbert        P106   \n",
            "4     Q2856873        Antoine de Fevin         P20   \n",
            "...        ...                     ...         ...   \n",
            "4862  Q3709870          Herby Fortunat        P413   \n",
            "4863   Q150804             The Pianist        P136   \n",
            "4864  Q1343857                Jon Seda         P19   \n",
            "4865  Q2981169       Nikolaj Frobenius        P106   \n",
            "4866      Q174               Sao Paulo         R19   \n",
            "\n",
            "                                               Question  \n",
            "0     Who was the trump ocean club international hot...  \n",
            "1                          where was sasha vujacic born  \n",
            "2          What is a film directed by wiebke carolsfeld  \n",
            "3            What was Seymour Parker Gilbert profession  \n",
            "4         in what french city did antoine de fevin die   \n",
            "...                                                 ...  \n",
            "4862  which position did herby fortunat play in foot...  \n",
            "4863       what kind of film is the pianist (2002 film)  \n",
            "4864                     where was jon seda given birth  \n",
            "4865               What is nikolaj frobenius profession  \n",
            "4866         who is a person that was born in sao paulo  \n",
            "\n",
            "[4867 rows x 4 columns]\n"
          ]
        }
      ]
    }
  ]
}
